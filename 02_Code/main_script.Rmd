---
title: "Markdown presentation"
author: "Loris Jeitziner, Olivia Fischer and Nieves Schwab"
date: "11/24/2020"
output:
  html_document: default
  pdf_document: default
---
## Emotional Arcs in TED Talks 

What does a great commercial, a sales pitch and a scientific presentation have in common? 

They all try to tell (or sell) a compelling story. 

Stories are appealing to us, as they use a tight and logical structure, they help us organize information. It has a logical progression, the classic story structure always looks the same pattern, first the context is set, and the hero is introduced. The hero has a problem or a challenge he has to solve. He struggles, then struggles more and finally he resolves the crises. Happy end. 

This ups and downs in the stories are an important element, they capture the audience's interest and take them on an emotional journey. Kurt Vonnegut, proposed that there exist six "universal shapes" of stories, or emotional arcs, that follow specific pattern in stories. 

We propose that emotion plays an important role in storytelling. And we wanted to analyze to which extent. To this purpose we have been looking at data from the masters of storytelling, the speakers of TED Talks. 

According to Wikipedia: TED conferences (Technology, Entertainment, Design) is a media organization that posts talks online for free distribution, under the slogan "ideas worth spreading". They address a wide range of topics within the research and practice of science and culture, often thorugh storytelling. The speakers are given a maximum of 18 minutes to present ideas in the most innovative and engaging way as they can. 

The dataset that we analyze contains 2,386 TED Talks from the years 1972 to 2017. It contains variables like the speaker, headline of the talk, duration, year of publication and... the number of views!

In the last two weeks we have been analyzing this dataset in a data-driven manner, exploring if emotion/valence has an influence on the number of views a talk has. For this purpose we have defined the variable of views as our DV, defining talks with more views are more successful. 

Additional variables contained in the dataset stem from LIWC. They are, for example: word count, tone, number of pos (e.g. ppron), affect,...

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(report)
library(tidyverse)
library(tidytext)
library(stringr)
library(stopwords)
library(textdata)
library(readr)
library(ggrepel) 
library(viridis)
library(stringr) 
library(grid)
library(wordcloud)
library(ClusterR) 
library(cluster) 
library(kmed)
library(pander)
library(cstab)
library(sentimentr)
library(caret)
library(party)
library(partykit)
library(lubridate)
library(netdiffuseR)
library(ggraph)
library(igraph)
library(qgraph)

#round numbers
options(scipen=999)

```

# Data preparation

- Reading of the transcripts and engagement values
- changing of formats
- Omitting non-youtube talks

```{r, message = F, warning=F}
#load data  

data = read_csv("../01_Data/TED_dirk.csv", locale = locale(encoding = "UTF-8"))
# data2 = read_csv("data/ted.csv", locale = locale(encoding = "UTF-8"))

# data = read_csv("data/ted.csv", locale = locale(encoding = "UTF-8"))
# data$posemo

# str_replace_all(data$duration, "[:alpha:]" , "")

#change duration to date/time format
#omit letters
data = data %>% mutate(
  duration = str_replace_all(data$duration, "[:alpha:]" , "")
)
#add colon
data = data %>% mutate(
  duration = str_replace_all(data$duration, "[:blank:]" , ":")
)
#define as time
data = data %>% mutate(
  duration = ms(data$duration)
)

#define time since upload
data = data %>% mutate(
  days_since_upload = difftime("2020-12-31", upload_date_youtube, units = "days")
)


#add likes/dislikes ratio
data = data %>% mutate(
  like_dislike_ratio = dislike/likes
)


#alle nicht youtube entfernen
data = data %>% filter(!is.na(views_youtube))

summary_by_event = data %>% group_by(event) %>% summarise(
  mean_views = mean(views, na.rm = FALSE),
  sd_views = sd(views, na.rm = FALSE)
)

table(complete.cases(data$views))

# unique(summary_by_event$event)
summary_by_event = summary_by_event %>% arrange(mean_views)
summary_by_event %>%  ggplot(aes(x = reorder(event, -mean_views), y = mean_views)) + 
  geom_point()+
  geom_point(aes(x = event, y = sd_views))

data %>% ggplot(aes(x = reorder(event, -views), y = log(views))) + 
  geom_boxplot()

events_number = as.data.frame(table(data$event))
colnames(events_number) = c("event","freq")
summary_by_event = summary_by_event %>% left_join(events_number)

```


```{r}

#mark all TED-Ed videos
data = data %>% mutate(
  ted_ed = case_when(event=="TED-Ed" ~ 1,
                     event!="TED-Ed" ~ 0
))


#exclude ted ed data
data = data %>% filter(ted_ed == 0)

# data %>% arrange(upload_date)

#Take a look at the data
data %>%  arrange(upload_date) %>% ggplot(show.legend = FALSE,aes(x = upload_date, y = log(views), color = event)) +
  geom_point(show.legend = FALSE) + 
  labs(x = "Upload Date", y = 'log(Views)') + 
  theme_minimal()+ scale_x_continuous(breaks = as.Date(c("2010-01-01", "2012-01-01", "2014-01-01", "2016-01-01", "2018-01-01","2020-01-01" )))

#It seems that at some point (end of 2017) the number of videos exploaded. also there are now more videos with less views
data %>% select(upload_date, views) %>% arrange(upload_date) %>% ggplot(aes(upload_date)) +
  geom_histogram() + 
  labs(x = "Upload Date") + 
  theme_minimal()

#are the variables views and views_youtube similar?
# both_views = as.data.frame(cbind(data$views, data$views_youtube))
# both_views = na.omit(both_views)
# cor(both_views$V1, both_views$V2)
  

```

# Tags analysis preparation

- Tokenization of Tags
- Adjencency Matrix of tags that occur with each other


```{r}
#create tokens from tags
tags_tbl = unnest_tokens(data, input = "tags", token = "words", to_lower = TRUE, output = "word")

#remove NA
tags_tbl = tags_tbl %>% filter(!is.na(word))

#extract unique tags
# unique(tags_tbl$word)

#create wordlist for adjencency matrix
wordlist = as.vector(unique(tags_tbl$word))

#see number of mentions
# as.list(sort(table(tags_tbl$word),decreasing = TRUE))

#omit redundant tags within talks
tags_tbl = tags_tbl %>% group_by(id) %>%  distinct(word)


#add numeration to tags_tbl
tags_tbl = tags_tbl %>% group_by(id) %>% mutate(n = row_number())


tags = tags_tbl %>% select(id, n, word)

# length(unique(tags$id))


# adj.1.1 beide kommen vor

#create empty adjecency matrix
adj = matrix(0, nrow = length(wordlist), ncol = length(wordlist), dimnames = list(wordlist))

#name columns and rows
colnames(adj) =  wordlist
rownames(adj) = wordlist
# 
# i = 2
# 
# l = 1
# n = 6
# m = 2
# log = ""

for (i in 1:length(unique(tags$id))){
    list = tags[(tags$id==i),3]
    list = as.vector(list$word)
    k = NULL
    if (length(list)>1){
      # for (k in 1:length(list)){
        n = length(list)
        l = m = NULL
        for (l in 1:(n-1)){
          for (m in (l+1):n){
           
            adj[list[l], list[m]] = adj[list[l], list[m]] + 1
            
            # log_txt=(c(
            #   list[l]," + ", list[m], " = ", adj[list[l], list[m]]
            #   ))
            
            # log_txt = str_c(log_txt, sep = "", collapse = "")
            # log = rbind(log, log_txt )
            
            
            adj[list[m], list[l]] = adj[list[m], list[l]] + 1
            
            
            
           }
        }
     # }
  }
}


###### NOTE: JACCARD COEFFICIENT CALCULATION takes a long time. therefore I already calculated them and saved the adjencency matrix as jaccard.csv in the data folder, the following code is still here to provide the information on how it was done ######


#read jaccard adjecency matrix
jaccard = read_csv(file="../01_Data/jaccard.csv")

rownames(jaccard) = jaccard$X1
jaccard = as.matrix(jaccard)
jaccard = jaccard[,-1]



# #### ADD JACCARD COEFFICIENT #####
# 
# # adj.1.1 beide kommen vor
# 
# #create empty adjecency matrix
# jaccard = matrix(0, nrow = length(wordlist), ncol = length(wordlist), dimnames = list(wordlist))
# 
# #name columns and rows
# colnames(jaccard) =  wordlist
# rownames(jaccard) = wordlist
# i = 1 
# j = 2
# 
# 
# for (i in 1:ncol(jaccard)){
#   for(j in 1:nrow(jaccard)){
#     if(adj[i,j]!=0){
#     intersection = adj[i,j]
#     n.x = tags_tbl %>% filter(word == colnames(adj)[i]) %>% nrow() 
#     n.x = n.x - intersection
#     
#     n.y = tags_tbl %>% filter(word == colnames(adj)[j]) %>% nrow() 
#     n.y = n.y - intersection
#     
#     jaccard[i,j] = intersection / (intersection + n.x + n.y)
#     }
#   }
# }
# write.csv(jaccard, file = "../01_Data/jaccard.csv")


#Prepare graph creation
graph = graph_from_adjacency_matrix(jaccard, mode = "undirected", weighted = TRUE)



```

# Tags analysis
- Louvain 
- Community detection
- Assign communities to talks


```{r}

# run louvain with edge weights 
louvain_partition <- igraph::cluster_louvain(graph, weights = E(graph)$weight) 
# assign communities to graph 
graph$community <- louvain_partition$membership 

table(louvain_partition$membership)

# see how many communities there are 
unique(graph$community) 




communities <- data.frame() 
for (i in unique(graph$community)) { 
# create subgraphs for each community 
  subgraph <- induced_subgraph(graph, v = which(graph$community == i)) 
# get size of each subgraph 
size <- igraph::gorder(subgraph) 
# get betweenness centrality 
btwn <- igraph::betweenness(subgraph) 
communities <- communities %>% 
  dplyr::bind_rows(data.frame(
    community = i, 
    n_tags = size, 
    most_important = names(which(btwn == max(btwn))) 
    ) 
  ) 
} 
knitr::kable(
  communities %>% 
    dplyr::select(community, n_tags, most_important)
)


top_five <- data.frame() 
for (i in unique(graph$community)) { 
  # create subgraphs for each community 
  subgraph <- induced_subgraph(graph, v =       which(graph$community == i)) 
  # for larger communities 
  if (igraph::gorder(subgraph) > 1) { 
    # get degree 
    degree <- igraph::degree(subgraph) 
    # get top five degrees 
    top <- names(head(sort(degree, decreasing = TRUE), 5)) 
    result <- data.frame(community = i, rank = 1:5, character = top) 
  } else { 
    result <- data.frame(community = NULL, rank = NULL, character = NULL) 
  } 
  top_five <- top_five %>% 
    dplyr::bind_rows(result) 
} 
knitr::kable(
top_five %>% 
  tidyr::pivot_wider(names_from = rank, values_from = character) 
)


#define main tags
main_tags = c("environment","society","technology","health","physics","entertainment","history","mind","marketing")


#### PLOTTING #####

# # give our nodes some properties, incl scaling them by degree and coloring them by community 
# V(graph)$size <- 1
# V(graph)$frame.color <- "white" 
# V(graph)$color <- graph$community 
# V(graph)$label <- V(graph)$name 
# V(graph)$label.cex <- 1
# 
# 
# # also color edges according to their starting node 
# edge.start <- ends(graph, es = E(graph), names = F)[,1] 
# E(graph)$color <- V(graph)$color[edge.start] 
# E(graph)$arrow.mode <- 0 
# 
# v_labels <- which(V(graph)$name %in% main_tags) 
# for (i in 1:length(V(graph))) { 
#   if (!(i %in% v_labels)) { V(graph)$label[i] <- "" } 
# }
# 
# 
# #plot network 1
# l1 <- layout_on_sphere(graph)
# plot(graph, rescale = T, layout = l1, main = "'Tags Communities of TED Talks")
# 
# 
# #plot network 2
# l2 <- layout_with_mds(graph) 
# plot(graph, rescale = T, layout = l2, main = "Tags Communities of TED Talks")
# 
# 
# # Infomap
# imc <- cluster_infomap(graph)
# membership(imc)
# communities(imc)
# plot(louvain_partition, graph)


# tags_communities=tags = NULL
#write communities back into tbl
tags_communities = as.data.frame(cbind(graph$community , wordlist)) %>% rename(word = wordlist)

tags = tags %>% inner_join(tags_communities) 
tags = tags %>% rename(community = V1)

#which tag is the most often used per talk
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
# main_tags = c("environment","society","technology","health","physics","entertainment","history","mind","marketing")


#create tag_chr
tags = tags %>% group_by(id) %>% mutate(
  community = as.numeric(community),
  main_tag = Mode(community),
  community_chr = case_when(
    main_tag == 1 ~ "environment",
    main_tag == 2 ~ "society",
    main_tag == 4 ~ "technology",
    main_tag == 3 ~ "health",
    main_tag == 5 ~ "physics",
    main_tag == 6 ~ "entertainment",
    main_tag == 7 ~ "history",
    main_tag == 8 ~ "mind",
    main_tag == 9 ~ "marketing",
  )
)

# table(tags$main_tag)



#back to data
id_community = tags %>% group_by(id) %>% summarise(
  community = mean(main_tag)
)

#create community_chr
data = data %>% inner_join(id_community)
data = data %>% mutate(
  community_chr = case_when(
    community == 1 ~ "environment",
    community == 2 ~ "society",
    community == 4 ~ "technology",
    community == 3 ~ "health",
    community == 5 ~ "physics",
    community == 6 ~ "entertainment",
    community == 7 ~ "history",
    community == 8 ~ "mind",
    community == 9 ~ "marketing",
  )
)
# table(data$community_chr)
# data$community

```


# The thing about the bad data after 2017

It seems, that after 2017 there was a surge in uploads of new TED talks, which also were mainly unsuccesfull in terms of views. A lot of the "new" data even misses information about views. 
Exluding TEDx talks and cutting of recent talks (since 2020) was a good method to "clean" the data. 


# Emotional Arcs in TED Talks

We are interested in discovering if there are distinct emotional arcs and if they can predict the number of views a TED Talk will have. To plot the sentiment arcs first we have to tokenize the transcripts of the talks and add the sentiment values. 

Tokenize the data and add sentiment

```{r, message = F, warning=F}
#tokenize the transcripts 
# data$transcript
token_tbl = unnest_tokens(data, input = "transcript", token = "words", to_lower = TRUE, output = "word")
# sentence_table = unnest_tokens(data, input = "transcript", token = "sentences", to_lower = TRUE, output = "sentence")


#senti Words
# sentiwords = read_csv("../01_Data/SentiWords_1.1.txt", locale = locale(encoding = "UTF-8"))
  

#vader -->  vader geht zu lange!!! Für 10 ted talks 2.5 min (extrapoliert auf 3000~talks -> 750 min)
# install.packages("vader")
# library(vader)
# vader_tbl = vader_df(data$transcript[1:10])
# get_vader(beispiel)


#sentiwordnet
# install.packages("lexicon")
library(lexicon)
sentiwords = hash_sentiment_sentiword
sentiwords = sentiwords %>% rename(value = y, word = x)
```



# Frequency of affect words
- Get affect words via sentiwords lexicon

```{r, message = F, warning=F}
#relative position of the word for the sentiment analysis -> arc, also add wordcount for LIWC analysis
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  word_count = n(),
  pos = 1:n(),
  rel_pos = (pos-1)/max(pos-1)
)
# citation(tidytext)

#load the sentiment word from the "afinn" repository
# afinn = get_sentiments("afinn")


# citation("afinn")
#join the tokens with the sentiment words
token_tbl <- inner_join(y = sentiwords,x = token_tbl)

# token_tbl = token_tbl %>% group_by(id) %>% mutate(
#   pos = 1:n(),
#   rel_pos = (pos-1)/max(pos-1)
# )

#add percentage of affect words
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  affect = (n()/word_count)
)
# token_tbl$affect
#add percentage of pos and neg words to the token_tbl
token_tbl_pos = token_tbl %>% filter(value>0) %>% group_by(id) %>% mutate(
  posemo = n()/word_count,
  negemo = 0
)
token_tbl_neg = token_tbl %>% filter(value<0) %>% group_by(id) %>% mutate(
  negemo = n()/word_count,
  posemo = 0
)

#bind both subsets
token_tbl = rbind(token_tbl_neg, token_tbl_pos)

#arrange by id and rel_pos
token_tbl = token_tbl %>% arrange(id, rel_pos)

#overwrite zeroes
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  negemo = max(negemo),
  posemo = max(posemo)
)

#add intensity, abs intensity and valence
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  mean_intensity = mean(value),
  abs_mean_intensity = abs(mean_intensity),
  valence = case_when(
    mean_intensity > 0 ~ "positive",
    mean_intensity < 0 ~ "negative"
  )
)


# token_tbl %>% select(word, word_count, pos, rel_pos, value, posemo, negemo, affect )


#add posemo and negemo back to the wide data set
posnegemo_wide = aggregate(id ~ posemo + negemo + affect + mean_intensity + abs_mean_intensity + valence, mean, data = token_tbl)
talks = data %>% left_join(posnegemo_wide, by = "id")

#see if it worked
# talks %>% select(id,affect,  posemo, negemo, views)

#remove NA's
talks = talks %>% filter(!is.na(mean_intensity))

#see correlation of independent variables
cor_table = talks %>% select(affect, posemo, negemo, mean_intensity, abs_mean_intensity) %>% mutate(
  dif = posemo - negemo
) %>% cor()

# talks %>% ggplot(aes(x = abs_mean_intensity, y = mean_intensity)) + geom_point()

```

# Tags descriptive analysis
- barplot of frequencies

```{r}


talks$community_chr = as.factor(talks$community_chr)
table(talks$community_chr)

data %>%  ggplot(aes(x = community_chr)) + geom_bar()

# data %>%  ggplot(aes(x = community_chr, y = views_youtube)) + geom_boxplot()


model_tags <- glm(views_youtube ~ affect + community + days_since_upload, family = "poisson", data = talks)
summary(model_tags)


# talks %>% filter(presenter == "Amy Cuddy") %>% select(tags, community)

# table(talks$community_chr)
```



# Analysis of outcome variables for composite outcome

```{r}

#define composite outcome variable - engagement - via combination formula
# engagement = (0.098 x Likes + 0.118 x Dislikes + 0.713 x Comments) / views
 
talks = talks %>% mutate(
  engagement_a = (0.106*likes + 0.127 * dislike + 0.767 * no_comments) / (views_youtube) ,
  engagement_b = (1/6*likes + 2/6 * dislike + 3/6 * no_comments) / (views_youtube),
  engagement_c = (1/3*likes +  1/3*dislike + 1/3*no_comments) / (views_youtube)

)
# hist(talks$engagement_c)

talks %>%  ggplot(aes(engagement_c, log(likes))) + geom_point()


outcomes = talks %>% select(views_youtube, likes, dislike, no_comments, engagement_c) %>% na.omit()

outcomes_log = outcomes %>%  mutate(
  likes = log(likes),
  dislike = log(dislike),
  no_comments = log(no_comments),
  views_youtube = log(views_youtube)
  
)


cor(outcomes_log)
corrplot::corrplot(cor(outcomes_log), method = "number")



```

 

# Preparation - Inference analysis of sentiment words and engagement data

```{r}
#poisson regression analysis of talks with frequency measure
# talks = data %>% left_join(posnegemo_wide, by = "id")

#remove na talks
talks = talks %>% filter(!is.na(affect), .preserve = TRUE)
talks = talks %>% filter(!is.na(views_youtube), .preserve = TRUE)
talks = talks %>% filter(!is.na(no_comments), .preserve = TRUE)

#Convert the variable "date_published" into an actual date 
talks$upload_date <- as.Date(talks$upload_date, format = "%y/%m/%d")

#Select all the talks published between 2006 and 2016. 
#We use date_published instead of year_filmed. 

talks %>% select(upload_date) %>%  arrange(desc(upload_date))

# talks = talks %>% filter(upload_date > "2005-12-31" & upload_date < "2019-01-01")


### analyses with log(views_youtube) or poisson ###
tilt_theme <- theme(axis.text.x=element_text(angle=45, hjust=1))
```




```{r}
#Descriptive analysis of relation between outcomes and predictors


# affective frequency using log(views_youtube)
af_freq_views <- ggplot(talks, aes(affect, log(views_youtube))) +
  geom_point() +labs(title= "Log(views_youtube) by % of affect words") + tilt_theme +
  labs(x = "% of affect words in a talk", y = "log(views_youtube)")+ geom_smooth(method = "lm")
af_freq_views

# affective frequency using log(views_youtube)
af_freq_engagement <- ggplot(talks, aes(affect, engagement_c)) +
  geom_point() +labs(title= "Engagement by % of affect words") + tilt_theme +
  labs(x = "% of affect words in a talk", y = "Engagement")+ geom_smooth(method = "lm")
af_freq_engagement

# affective frequency using log(views_youtube)
ave_intensity_engagement <- ggplot(talks, aes(abs_mean_intensity, engagement_c)) +
  geom_point() +labs(title= "Engagement by mean_intensity") + tilt_theme +
  labs(x = "mean intensity", y = "Engagement")+ geom_smooth(method = "lm")
ave_intensity_engagement


```


# Inference analysis of sentiment words and engagement data


```{r}
# poisson regression views
m_views <- glm(views_youtube ~ affect + mean_intensity + poly(days_since_upload, 2), family = "poisson", data = talks)
summary(m_views)

# add predicted log(views_youtube) to talks
prediction = predict(m_views)
talks = talks %>% mutate(pred_m_views = prediction)


# plot the points (actual observations), regression line
plot_m_views <- ggplot(talks, aes(affect,log(views_youtube))) + 
  geom_point() +
  geom_smooth(aes(affect, pred_m_views)) +
  labs(title= "Log(views_youtube) by % of affect words", x = "% of affect words in a talk", y = "log(views_youtube)")
plot_m_views
```


```{r}

m_engagegement <- lm(engagement_c ~ mean_intensity + affect + community_chr +  poly(days_since_upload, 2), data = talks)
summary(m_engagegement)
report(m_engagegement)


# talks <-   cbind(talks, pred_m_views = prediction)
prediction = predict(m_engagegement)
talks = talks %>% mutate(pred_m_engagement = prediction)

# plot the points (actual observations), regression line
plot_m_engagement <- ggplot(talks, aes(affect, engagement)) + 
  geom_point() +
  geom_smooth(aes(affect, pred_m_engagement)) +
  labs(title= "engagement by % of affect words", x = "% of affect words in a talk", y = "engagement")
plot_m_engagement

# hist(talks$affect)
```


```{r}
# poisson regression no_comments
m_comments <- glm(no_comments ~ affect + mean_intensity + poly(days_since_upload, 2), family = "poisson", data = talks)
summary(m_comments)
# range(talks$no_comments)

# add predicted log(no_comments) to talks
prediction = predict(m_comments)
talks = talks %>% mutate(pred_m_comments = prediction)

# plot the points (actual observations), regression line
plot_m_comments <- ggplot(talks, aes(affect,log(no_comments))) + 
  geom_point() +
  geom_smooth(aes(affect, pred_m_comments)) +
  labs(title= "Log(no_comments) by % of affect words", x = "% of affect words in a talk", y = "log(no_comments)")

plot_m_comments

```



```{r}
# # positive emotion
# pos <- ggplot(talks, aes(posemo, log(views_youtube))) +
#   geom_point() +labs(title= "Log(views_youtube) by % of positive affect words") + tilt_theme
# 
# pos 
# 
# # talks %>% select(views_youtube, affect) %>% filter(!is.na(views_youtube))
# 
# 
# # poisson regression of positive affect
# poiss_pos <- glm(views_youtube ~ posemo + days_since_upload, family = "poisson", data = talks)
# poiss_pos2 <- glm(views_youtube ~ posemo, family = "poisson", data = talks)
# 
# summary(poiss_pos)
# 
# #the chisq test shows the significance of days since upload as a predictor and even better fit! 
# anova(poiss_pos2, poiss_pos, test = "Chisq")
# 
# # add predicted log(views_youtube) to talks
# talks <- cbind(talks, pred_posemo = predict(poiss_pos))
# 
# 
# 
# # posemo plot the points (actual observations), regression line
# plot_posemo <- ggplot(talks, aes(posemo, log(views_youtube))) + 
#   geom_point() +
#   geom_smooth(aes(posemo, pred_posemo)) +
#   labs(title= "Log(views_youtube) by % of positive affect words", x = "% of positive affect words in a talk", y = "log(views_youtube)")
# 
# plot_posemo
```





Smooth sentiment arcs

```{r}
# smoothing function for smoothing the sentiment values relative to the position
smooth = function(pos, value){ 
  sm = sapply(pos, function(x) {
    weights = dnorm(pos, x, max(pos) / 10)
    sum(value * (weights / sum(weights)))
    })
  }

# define the sentiment values as sent_values per talk
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  sent_values = smooth(pos, value)
  # z_sent_values = scale(sent_values)
)

#z standardize the sentiment values
token_tbl$z_sent_values = scale(token_tbl$sent_values)
```



Sentiment Arcs for the most viewed talks 
```{r}
# plot sentiment arcs not z transformed to compare, color by title
sent_top_views <- token_tbl %>% filter(views >= 18253944) %>% ggplot(aes(rel_pos, sent_values, color= title)) +geom_line(lwd=2) +  labs(x = "Position", y = 'Sentiment',  title= "Emotional Arcs for the 10 most viewed TED Talks") + theme_minimal()
sent_top_views
```

Sentiment Arcs for the less viewed talks 
```{r}
sent_last_views <- token_tbl %>% filter(views <= 218993) %>% ggplot(aes(rel_pos, sent_values, color= title)) +geom_line(lwd=2) +  labs(x = "Position", y = 'Sentiment',  title= "Emotional Arcs for the 10 less viewed TED Talks") + theme_minimal()
sent_last_views
```


# Chunking and analysis of clusters to predict number of views


```{r include=FALSE, echo=FALSE}
library(cluster)
#reduce dataset to necessary variables 
data_cut = token_tbl %>% select(id, presenter,views, sent_values, z_sent_values, value, rel_pos, pos, word)

#make chunks
n_chunk = 5

data_cut = data_cut %>% group_by(id) %>% mutate(
  chunk = round(rel_pos/n_chunk, 2)*100
)


#make means per chunk
data_cut = data_cut %>% group_by(id) %>% group_by(chunk) %>% mutate(
  mean_chunk = mean(z_sent_values),
  mean_rel = mean(rel_pos)
)

#Cluster analyse
#Korrelationen
data_agg = aggregate(data=data_cut, z_sent_values ~ id + chunk, mean)
colnames(data_agg) = c("id", "chunk","z_sent_values")
data_agg$z_sent_values = as.numeric(data_agg$z_sent_values)
data_agg$id = as.character(data_agg$id)

data_agg = data_agg %>% group_by(id) %>% mutate(
  n_group = n()
)

data_agg = data_agg %>% filter(n_group==21)

data_agg = data_agg %>% group_by(id) %>% mutate(
  n_group = n()
)

# select and widen
data_agg_2 = data_agg %>% select(id, z_sent_values)


#wrangle data into matrix
data_agg$z_sent_values = as.numeric(data_agg$z_sent_values)
data_agg_2 = as.data.frame(data_agg_2)
data_agg_2 = data_agg_2 %>% select(id, z_sent_values)
data_agg_2 = data_agg_2 %>% group_by(id) %>% mutate(row = row_number())
data_wide = data_agg_2 %>%pivot_wider(names_from=id, values_from=z_sent_values) %>% select(-row)

#make it to a matrix
data_wide = as.matrix(data_wide)

#transpose the matrix for the calculation of euclidian distances
t_wide = t(data_wide)
```

We used the daisy function for calculating euclidian distances. Furthermore, with the k-mediods approach, we built clusters.

```{r include=TRUE, echo=TRUE}
#setting seed for clustering
set.seed(240)  

#number of possible cluster numbers
kseq=c(2:75)

#find out number of clusters with cstab
distance = cDistance(t_wide, kseq, method = "hierarchical", linkage = "ward.D",
kmIter = 10, gapIter = 10)

plot(distance$Jumps)
plot(distance$Gaps)
plot(distance$Slopes)



#calculate euclidian distances
dist_data <- daisy(t_wide, stand=TRUE)

# cluster analysis with fast k mediod method -> 10 clusters
# cluster_analysis_10 <- fastkmed(dist_data, ncluster = 10, iterate = 5000)
# cluster_analysis <- fastkmed(dist_data, ncluster = 8, iterate = 5000)
# cluster_analysis <- pam(dist_data, k=24 )

cluster_analysis <- diana(dist_data, diss=TRUE, metric="euclidean")


# Es wird das Kriterium "height" in Abhängigkeit der letzten 10 Fusionierungsschritte dargestellt.
height <- sort(cluster_analysis$height)
Schritt <- (length(height)-9):length(height)
height <- height[(length(height)-9):length(height)]
data4 <- data.frame(Schritt, height)
# library(ggplot2)
ggplot(data4, aes(x=Schritt, y=height)) + geom_line() 

```


```{r include=TRUE, echo=FALSE}
# Vektor "Cluster" mit Clusterzugehörigkeit
cluster <-  cutree(as.hclust(cluster_analysis), k = 18)  # Clusteranzahl k anpassen
cluster = as.data.frame(cluster)
ids = rownames(cluster)
ids = as.data.frame(ids)
cluster$id = ids

# implement cluster information in token dataframe
# cluster_id = cluster_analysis$clustering
# cluster = cluster_analysis[["clustering"]]
# cluster = as.data.frame(cluster)


colnames(cluster) = c("cluster", "id")

cluster$id = as.numeric(unlist(cluster$id))

# uni_ids = unique(token_tbl$id)
# uni_ids2 = unique(cluster$id)


#add cluster info to token_tbl
token_tbl_w_cluster <- token_tbl %>%
  left_join(cluster, by = "id")

# token_tbl_w_cluster[,120:128]

token_tbl_w_cluster = token_tbl_w_cluster %>% filter(!is.na(cluster))

```

The clusters actually show some tendencies, that there are multiple arcs in TED - Talks. 

```{r visualize clusters, include=TRUE, echo=FALSE}

token_tbl %>%   ggplot(aes(rel_pos, sent_values)) +
  geom_line(size = 1) + 
  labs(x = "Position", y = 'Sentiment') + 
  theme_minimal() + ylim(-2, 2)

token_tbl_w_cluster %>%   ggplot(aes(rel_pos, z_sent_values,group = id, color=cluster)) +
  geom_line(size = 0.1) + 
  labs(x = "Position", y = 'Sentiment') + 
  theme_minimal() + facet_wrap(. ~ cluster)


token_tbl_w_cluster %>% filter(cluster==1 | cluster==14 | cluster ==5 | cluster ==7 | cluster ==13 | cluster == 12) %>% ggplot(aes(rel_pos, z_sent_values,group = id, color=cluster)) +
  geom_line(size = 0.1) + 
  labs(x = "Position", y = 'Sentiment') + 
  theme_minimal() + facet_wrap(. ~ cluster)

token_tbl_w_cluster %>% filter(cluster==1) %>% ggplot(aes(rel_pos, z_sent_values,group = id, color=id)) +
  geom_line(size = 0.1) + 
  labs(x = "Position", y = 'Sentiment') + 
  theme_minimal() 

token_tbl_w_cluster$cluster = as.factor(token_tbl_w_cluster$cluster)

token_tbl_w_cluster%>% ggplot(aes(x = reorder(cluster, -log(views)), y  = log(views))) +
  geom_boxplot() + 
  labs(x = "Cluster", y = 'log(views)') + 
  theme_minimal() 


token_tbl_w_cluster %>% filter(cluster==13 | cluster ==15  | cluster ==18 | cluster== 12) %>% ggplot(aes(x = reorder(cluster, -log(views)), y  = log(views))) +
  geom_boxplot() + 
  labs(x = "Cluster", y = 'log(views)') + 
  theme_minimal() 

token_tbl_w_cluster %>% filter(cluster==13 | cluster ==15 | cluster ==18 | cluster== 12) %>% ggplot(aes(rel_pos, z_sent_values,group = id, color=id)) +
  geom_line(size = 0.1) + 
  labs(x = "Position", y = 'Sentiment') + 
  theme_minimal() + facet_wrap(.~cluster)

```


## Let's take a closer look at two clusters.

Cluster 7 shows more of a positive linear function, whereas cluster 2 looks like a negative quadratic function.

```{r visualize clusters2, include=TRUE, echo=FALSE}

# token_tbl_w_cluster$mean_sent_clust_pos  


# token_tbl_w_cluster %>% ggplot(aes(rel_pos, z_sent_values,group = id,color=cluster)) +
#   geom_line(size = 0.1) + labs(x = "Position", y = 'Sentiment') + 
#   theme_minimal() + facet_wrap(. ~ cluster) 

# + geom_smooth(method = "auto", se=FALSE , color="red")


token_tbl_w_cluster %>% filter(cluster==2) %>%  ggplot(aes(rel_pos, z_sent_values,group = id,color=id)) +
  geom_line(size = 0.1) + labs(x = "Position", y = 'Sentiment') + 
  theme_minimal() 

#add mean sentiment value per talk
token_tbl_w_cluster = token_tbl_w_cluster %>% group_by(id) %>% mutate(
  mean_sent = mean(z_sent_values)
)
#Durchschnittsark berechnen für jeden cluster

```


## What's the distribution of views in the seperate clusters?



```{r include=TRUE, echo=FALSE}
# install.packages("sandwich")
# install.packages("msm")



require(sandwich)
require(msm)
pred = token_tbl_w_cluster %>% select(id, cluster, views, mean_sent)


pred = aggregate(data = pred, id ~ cluster + views + mean_sent, mean)
pred$id = as.factor(pred$id)
pred$cluster = as.factor(pred$cluster)


model1 = glm(formula = views ~ cluster + mean_sent, data = pred, family = "poisson")
model0 = glm(formula = views ~ 1, data = pred, family = "poisson")


cov.m1 <- vcovHC(model1, type="HC0")
std.err <- sqrt(diag(cov.m1))
r.est <- cbind(Estimate= coef(model1), "Robust SE" = std.err,
"Pr(>|z|)" = 2 * pnorm(abs(coef(model1)/std.err), lower.tail=FALSE),
LL = coef(model1) - 1.96 * std.err,
UL = coef(model1) + 1.96 * std.err)

r.est

with(model1, cbind(res.deviance = deviance, df = df.residual,
  p = pchisq(deviance, df.residual, lower.tail=FALSE)))



anova(model1, model0)

# summary(model0)
summary(model1)

## create the plot
pred %>% ggplot( aes(y = log(views), x = reorder(cluster, -log(views)), colour = cluster)) +
  geom_point(aes(), alpha=.5, position=position_jitter(h=.2)) +
  labs(x = "", y = "")

pred %>%  ggplot(aes(log(views) ,color=cluster, fill=cluster)) +
  geom_histogram() + 
  labs(x = "Views", y = 'Count') + 
  theme_minimal() + facet_wrap(. ~ cluster)
```

*Conclusion:* They are still heavily skewed.

## But do clusters predict number of views?

```{r include=TRUE, echo=FALSE}

#Effektstärke, Modellgütevergleich, 




```

*Conclusion:* This preliminary analysis actually indicates, that cluster 7 significantly differs form the other clusters. But do we believe, that some clusters are more predictive of success (numbers of views) than others? *NO*


## Possible reason for this outcome

```{r include=TRUE, echo=FALSE}
pred %>% filter(cluster ==18)  %>%  ggplot(aes(views ,color=cluster, fill=cluster)) +
  geom_histogram() + 
  labs(x = "Views", y = 'Count') + 
  theme_minimal() + facet_wrap(. ~ cluster)
```

*Conclusion:* The data is heavily skewed. In Cluster 7 we see some really viral TED talks, which most likely explains the results of the linear model. 




