---
title: "TED_sentiment"
author: "Jeitziner Loris"
date: "02/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(tidytext)
library(dplyr)
library(stringr)
library(stopwords)
library(ggplot2)
library(textdata)


#round numbers
options(scipen=999)

```



# Load texts and inner join them

```{r}
#load data
data = read.csv(file="data/ted.csv", encoding = "UTF-8")


```

# Tokenize and add sentiment

```{r}

#exclude NA talks
data = data %>% filter(!transcript=="N/A", .preserve = TRUE)

#rename columns with strange names
data = data %>% rename(id = X.U.FEFF.id)
data = data %>% rename(views=views_as_of_06162017)

#tokenize the transcripts 
token_tbl = unnest_tokens(data, input = "transcript", token = "words", to_lower = TRUE, output = "word")



#relative position of the word for the sentiment analysis -> ark
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  pos = 1:n(),
  rel_pos = (pos-1)/max(pos-1)
)


#load the sentiment word from the "afinn" repository
afinn = get_sentiments("afinn")

#join the tokens with the sentiment words
token_tbl <- inner_join(y = afinn,x = token_tbl)



```

```{r}
# smoothing function for smoothing the sentiment values relative to the position
smooth = function(pos, value){ 
  sm = sapply(pos, function(x) {
    weights = dnorm(pos, x, max(pos) / 10)
    sum(value * (weights / sum(weights)))
    })
  }

# define the sentiment values as sent_valuees per talk
token_tbl = token_tbl %>% group_by(id) %>% mutate(
  sent_values = smooth(pos, value)
)

```


# Plot sentiment arks

## the following plots illustrate examples for TED talks from Al Gore.

```{r}
# plot sentiment arcs not z transformed to compare, color by title
token_tbl %>% filter(speaker=="Al Gore"  ) %>% ggplot(aes(rel_pos, sent_values,color=event)) +
  geom_line(lwd=2) + 
  labs(x = "Position", y = 'Sentiment') + 
  theme_minimal()


```

# Calculate models for arcs


The idea here is, that all 6 arcs can be illustrated as models.
The first two arcs are linear models (positive/negative). 
The third and forth are quadratic (positive/negative), and the last two are cubic (positive/negative) or sinus/cosinus functions (still unclear)

We just have to calculate all the models for each sentiment sequence to see, which model fits best. 
Then we have the best model for each TED talk.

```{r}
# remove NA talks
token_tbl = token_tbl[!is.na(token_tbl$sent_values),]


# unique IDs in vector for loop through talks
talks = unique(token_tbl$id)

#create matrix for r-squared values
model_matrix = as.data.frame(matrix(data=NA, nrow= length(talks), ncol = 1))

#rename the rownames in the model matrix by the talks ID's
rownames(model_matrix)=talks


# -> zusätzlich noch steigung (beta) in model_matrix einfügen, damit wir zwischen negativ/positiv unterscheiden können


#loop through talks an calculate models
#extract r squared values und write them into the model matrix
for(i in talks){
  #linear pos
  x=token_tbl[token_tbl$id==i,"rel_pos"][[1]]
  y=token_tbl[token_tbl$id==i,"sent_values"][[1]]
  # print(i)
  model = lm(formula="y ~ x")
  model_matrix[i,1]= summary(model)[["r.squared"]][1]
  
  #quadratisch
  model2 = lm(y ~ x + I(x^2))
  model_matrix[i,2]= summary(model2)[["r.squared"]][1]
  
  #kubisch -> möglichkeit abzunändern zu sinus/cosinus funktion
  # x3 = poly(x,degree=3))
  model3 = lm(y ~ x + I(x^2) + I(x^3))
  model_matrix[i,3]= summary(model3)[["r.squared"]][1]
}

# token_tbl %>% group_by(id) %>% filter(id==179) %>% select(z_sent_values)
```


```{r}
# round r squared
model_matrix = round(model_matrix, 5)

# compare latest models as an example
anova(model, model2)
anova(model2, model3)
anova(model, model3)

#plot 
plot(x,y)


```

